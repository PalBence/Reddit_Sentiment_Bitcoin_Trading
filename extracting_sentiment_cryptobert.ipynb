{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the raw sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CryptoBERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ElKulako/cryptobert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ElKulako/cryptobert\")\n",
    "\n",
    "# Create a sentiment analysis pipeline using CryptoBERT\n",
    "cryptobert_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=DEVICE)\n",
    "\n",
    "post_1 = \" see y'all tomorrow and can't wait to see ada in the morning, i wonder what price it is going to be at. ðŸ˜ŽðŸ‚ðŸ¤ ðŸ’¯ðŸ˜´, bitcoin is looking good go for it and flash by that 45k. \"\n",
    "post_2 = \"  alright racers, itâ€™s a race to the bottom! good luck today and remember there are no losers (minus those who invested in currency nobody really uses) take your marks... are you ready? go!!\" \n",
    "post_3 = \" i'm never selling. the whole market can bottom out. i'll continue to hold this dumpster fire until the day i die if i need to.\" \n",
    "df_posts = [post_1, post_2, post_3]\n",
    "preds = cryptobert_pipeline(df_posts, return_all_scores=True)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for truncating if the message is too long for CryptoBERT\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "def truncate_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) > MAX_TOKENS:\n",
    "        tokens = tokens[:MAX_TOKENS]\n",
    "        return tokenizer.convert_tokens_to_string(tokens)\n",
    "    return text\n",
    "\n",
    "def batched_sentiment_pipeline(texts, batch_size=16):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        batch = [truncate_text(x) for x in batch]  # <- manual truncation here\n",
    "\n",
    "        try:\n",
    "            output = cryptobert_pipeline(batch, return_all_scores=True)\n",
    "            results.extend(output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            fallback = [[{'label': 'LABEL_0', 'score': 0}, {'label': 'LABEL_1', 'score': 0}, {'label': 'LABEL_2', 'score': 0}]] * len(batch)\n",
    "            results.extend(fallback)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the sentiment scores for each comment\n",
    "\n",
    "comment_df_paths = os.listdir('./comments/')\n",
    "\n",
    "for comment_df_path in tqdm(comment_df_paths, desc=\"Files\"):\n",
    "    comment_df = pd.read_csv(f'./comments/{comment_df_path}')\n",
    "    comment_df.dropna(subset=['text'], inplace=True)\n",
    "\n",
    "    texts = comment_df['text'].tolist()\n",
    "    sentiments = batched_sentiment_pipeline(texts)\n",
    "\n",
    "    # Store sentiment score columns\n",
    "    comment_df['sentiment'] = sentiments\n",
    "    comment_df['positive_score'] = [s[2]['score'] for s in sentiments]\n",
    "    comment_df['negative_score'] = [s[0]['score'] for s in sentiments]\n",
    "    comment_df['neutral_score']  = [s[1]['score'] for s in sentiments]\n",
    "\n",
    "    if os.path.exists('./comments_with_sentiment'):\n",
    "        comment_df.to_csv(f'./comments_with_sentiment/{comment_df_path}', index=False)\n",
    "    else:\n",
    "        os.makedirs('./comments_with_sentiment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the sentiment for each post\n",
    "\n",
    "post_df_paths = [f for f in os.listdir('./filtered_dfs/') if f.endswith('.csv')]\n",
    "post_dfs = []\n",
    "\n",
    "for post_df_path in tqdm(post_df_paths, desc=\"Processing post files\"):\n",
    "    post_df = pd.read_csv(f'./filtered_dfs/{post_df_path}')\n",
    "    post_df.dropna(subset=['title'], inplace=True)\n",
    "    post_df.drop(columns=['contains_keyword'], inplace=True, errors='ignore')\n",
    "    \n",
    "    subreddit = post_df_path.split(\"_\")[1] if 'wallstreetbets' not in post_df_path else 'wallstreetbets'\n",
    "    post_df['subreddit'] = subreddit\n",
    "\n",
    "    titles = post_df['title'].tolist()\n",
    "    sentiments = batched_sentiment_pipeline(titles)\n",
    "\n",
    "    post_df['sentiment'] = sentiments\n",
    "    post_df['positive_score'] = [s[2]['score'] for s in sentiments] \n",
    "    post_df['negative_score'] = [s[0]['score'] for s in sentiments]\n",
    "    post_df['neutral_score']  = [s[1]['score'] for s in sentiments]\n",
    "\n",
    "    post_dfs.append(post_df)\n",
    "\n",
    "# Merge all post DataFrames\n",
    "merged_post_df = pd.concat(post_dfs, ignore_index=True)\n",
    "merged_post_df.reset_index(drop=True, inplace=True)\n",
    "if os.path.exists('posts_with_sentiment'):\n",
    "    merged_post_df.to_csv('./posts_with_sentiment/merged_post_df.csv', index=False)\n",
    "else:\n",
    "    os.makedirs('./posts_with_sentiment')\n",
    "    merged_post_df.to_csv('./posts_with_sentiment/merged_post_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "university",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
